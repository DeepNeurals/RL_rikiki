So with:
-3 cards
- reward: 
good:5*pli's 
bad: -2 * error 
--> It learned the strategy of saying zero. With 3 cards saying zero is smart because you have 25% of playing zero, and winning 5 points. 25% of losing -2, 25% of losing -4 and 25% of losing -6



TEstiong of inference:
Given a state: state = [2, 1, 0, 2, 3, 0, 0, 0]  # 2 ACES, 1 KING, 2 ATOUTS, no other bid 


The model outputs:
Model predictions: tensor([[ -9.4445,  27.1933, -10.4334, -14.2121]])
Model BID pred: 1

The model always outputs 1 because that is what it learn did give him the highest reward
==> type of overfitting, the problem is too simple too solve. It learned that the best approach is zaying zero to accumulate rewards. 

I am gonna penalise the errors. 

We are gonna define better rewards:
- so: 1 game is gonna be: 4 rounds. In every round we play 3 tricks. After 4 round we give a reward if the AI-agent does well in function of the ranking

-Plot the distribution of actual tricks
-Implement such that another player becomes an AI player



The absoluate final objective is to develop a model, that given the state can predict the best bid to win the game. 
