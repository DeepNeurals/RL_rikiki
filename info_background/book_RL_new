An episode: the resulting trajectory from start to terminal state is called an episode of trial. 

terminal states as: - absorbing states; reduce the action space at that terminal state
			- normal state; but give a reward for staying in it
			
			
			
So basically the idea of the bidding model:
- learn the optimal bid to make in function of the number of cards, based on learned games we can come up with a number: for 2 cards upto 8 cards. 

--> I believe Rikiki is a stationary problem, the rules do not evolve over time. 

The reward depends on: R(s,a) and s'. But since s' depends on s, a we can rewrite it like that. 


--> Using the founded model information we will use this optimal bid, and learn the AI to play accordingly to win. 
Summarised: we want to learn the AI to win when needed, and lose when needed. 

If I can show that my model, given a hand of cards is able to play to lose and to win, we can conclude that the model learned how to play well. 


https://github.com/MathFoundationRL/Book-Mathematical-Foundation-of-Reinforcement-Learning/tree/main


Q-value: only 1 number? -> the state is a 1x9 tensor? Does that make sense??

The analysis shows that the model learn to tell one: Indeed on the long run, the true bid is often between 1.0 and 1.2. 
By saying 1 the AI-agent has the biggest chance of winning. Even in a game with 8 cards in the hand, the average true bid is only 1.8. 
The model could learn to say the best bid at every specific round, but that is not happening. The output is not related to the state. 
Using Q-learning is was thus found that the action that gives the highest reward is bidding:1. 


What I wanted to check now is if the average shifts with more conservative players or not. For this I will make all the players bid like a conservative player. 
==> We observe that when all the players play very conservatively, the AI agent bids higher on average. 

--> We can conclude that the model needs a bigger memory, such that it can understand these shifts faster and per round. 

-> Lets try to complexify the model: Does not change a lot!

After that we will try to improve the Q-value update, need to read chapter 2 for that. 


# After training, the optimal policy can be derived from the neural network
def optimal_policy(state):
    state = torch.FloatTensor(state).unsqueeze(0)
    with torch.no_grad():
        q_values = model(state)
    return np.argmax(q_values.numpy())
   
   --> Stayed at page 31


Come back onto comparing my model with the RL_snake model
long memory
short memory: only works for this scenario where consecutive good actions are taken

If I want to apply that I should consider storing benefit actions in some rounds
--------------------------------------------



For the report:
As can be seen in image: /home/tdebacker/rl_snake/rikiki_ws/image_outputs/plot_20240826_172118.png
The average true bid is around 1 in a game going until 8 cards with 4 players. By bidding 1 the AI-agent learned over multiple gains that bidding close to this line is a good strategy. 
What was also analysed is the playing behaviour of the other players and how this affect the playing strategy of the AI player. 



/home/tdebacker/rl_snake/rikiki_ws/image_outputs/plot_20240826_185800.png --> result with liberal, cons, random bidder (average is higher)
sensitivity using different gaming profiles

/home/tdebacker/rl_snake/rikiki_ws/image_outputs/plot_20240826_190155.png --> giving higher rewards (50 per pli), the average action is higher

With Playing, only following the existing reward:
/home/tdebacker/rl_snake/rikiki_ws/image_outputs/plot_20240827_081541.png --> 63% winning on 300 games, we want to increase that. 

/home/tdebacker/rl_snake/rikiki_ws/image_outputs/plot_20240827_094930.png --> reducing learning rate to 0.001
Onlyproblem: winning is defined as soring more than zero. 

Implementing this: /home/tdebacker/rl_snake/rikiki_ws/play_model_outputs/model20240827_111253_500.pth 







