So with:
-3 cards
- reward: 
good:5*pli's 
bad: -2 * error 
--> It learned the strategy of saying zero. With 3 cards saying zero is smart because you have 25% of playing zero, and winning 5 points. 25% of losing -2, 25% of losing -4 and 25% of losing -6



TEstiong of inference:
Given a state: state = [2, 1, 0, 2, 3, 0, 0, 0]  # 2 ACES, 1 KING, 2 ATOUTS, no other bid 


The model outputs:
Model predictions: tensor([[ -9.4445,  27.1933, -10.4334, -14.2121]])
Model BID pred: 1

The model always outputs 1 because that is what it learn did give him the highest reward
==> type of overfitting, the problem is too simple too solve. It learned that the best approach is zaying zero to accumulate rewards. 

I am gonna penalise the errors. 

